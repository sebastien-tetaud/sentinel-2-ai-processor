{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ccb655",
   "metadata": {},
   "source": [
    "## Get Sentinel 2 Data on CDSE using ODATA and Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2e9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pystac_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from loguru import logger\n",
    "import time\n",
    "import rasterio\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Now import the module\n",
    "from src.auth.auth import S3Connector\n",
    "from src.utils.utils import remove_last_segment_rsplit\n",
    "from src.utils.cdse_utils import (create_cdse_query_url, download_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a8dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cdse_query_url(\n",
    "    collection_name=\"SENTINEL-2\",\n",
    "    product_type=\"MSIL2A\",\n",
    "    polygon=None,\n",
    "    start_interval=None,\n",
    "    end_interval=None,\n",
    "    max_cloud_cover=100,\n",
    "    max_items=1000,\n",
    "    additional_filters=None,\n",
    "    orderby=\"ContentDate/Start\"  # Add orderby parameter with default value\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a query URL for the Copernicus Data Space Ecosystem OData API.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    collection_name : str\n",
    "        The collection name (e.g., 'SENTINEL-2', 'SENTINEL-1')\n",
    "    product_type : str\n",
    "        The product type (e.g., 'MSIL2A', 'MSIL1C', 'GRD')\n",
    "    polygon : str\n",
    "        WKT polygon string for spatial filtering\n",
    "    start_interval : str\n",
    "        Start time in ISO format with Z for UTC (e.g., '2023-01-01T00:00:00.000Z')\n",
    "    end_interval : str\n",
    "        End time in ISO format with Z for UTC (e.g., '2023-01-31T23:59:59.999Z')\n",
    "    max_cloud_cover : int\n",
    "        Maximum cloud cover percentage (0-100)\n",
    "    max_items : int\n",
    "        Maximum number of items to return\n",
    "    additional_filters : list\n",
    "        List of additional filter strings to add to the query\n",
    "    orderby : str or None\n",
    "        Field to order results by (e.g., 'ContentDate/Start', 'ContentDate/Start desc')\n",
    "        Set to None to skip ordering\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete URL for the OData API query\n",
    "    \"\"\"\n",
    "\n",
    "    # Basic filter for collection\n",
    "    filter_parts = [f\"Collection/Name eq '{collection_name}'\"]\n",
    "\n",
    "    # Add spatial filter if provided\n",
    "    if polygon:\n",
    "        filter_parts.append(f\"OData.CSC.Intersects(area=geography'SRID=4326;{polygon}')\")\n",
    "\n",
    "    # Add product type filter\n",
    "    if product_type:\n",
    "        filter_parts.append(f\"contains(Name,'{product_type}')\")\n",
    "\n",
    "    # Add temporal filters if provided\n",
    "    if start_interval:\n",
    "        filter_parts.append(f\"ContentDate/Start gt {start_interval}\")\n",
    "    if end_interval:\n",
    "        filter_parts.append(f\"ContentDate/Start lt {end_interval}\")\n",
    "\n",
    "    # Add cloud cover filter if applicable\n",
    "    # Only add for optical sensors (Sentinel-2)\n",
    "    if collection_name == 'SENTINEL-2' and max_cloud_cover < 100:\n",
    "        filter_parts.append(\n",
    "            f\"Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and \"\n",
    "            f\"att/OData.CSC.DoubleAttribute/Value le {max_cloud_cover})\"\n",
    "        )\n",
    "\n",
    "    # Add any additional filters\n",
    "    if additional_filters:\n",
    "        filter_parts.extend(additional_filters)\n",
    "\n",
    "    # Construct the URL with all filters\n",
    "    filter_string = \" and \".join(filter_parts)\n",
    "    url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter={filter_string}\"\n",
    "\n",
    "    # Add orderby parameter if specified\n",
    "    if orderby:\n",
    "        url += f\"&$orderby={orderby}\"\n",
    "\n",
    "    # Add top parameter for limiting results\n",
    "    url += f\"&$top={max_items}\"\n",
    "\n",
    "    url += \"&$expand=Attributes\"\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d3e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Now import the module\n",
    "from src.auth.auth import S3Connector\n",
    "from src.utils.utils import extract_s3_path_from_url\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "ACCESS_KEY_ID = os.environ.get(\"ACCESS_KEY_ID\")\n",
    "SECRET_ACCESS_KEY = os.environ.get(\"SECRET_ACCESS_KEY\")\n",
    "ENDPOINT_URL = 'https://eodata.dataspace.copernicus.eu'\n",
    "ENDPOINT_STAC = \"https://stac.dataspace.copernicus.eu/v1/\"\n",
    "DATASET_VERSION = \"V3\"\n",
    "BUCKET_NAME = \"eodata\"\n",
    "BASE_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor\"\n",
    "DATASET_DIR = f\"{BASE_DIR}/{DATASET_VERSION}\"\n",
    "BANDS = ['TCI']\n",
    "\n",
    "connector = S3Connector(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    access_key_id=ACCESS_KEY_ID,\n",
    "    secret_access_key=SECRET_ACCESS_KEY,\n",
    "    region_name='default'\n",
    ")\n",
    "# Get S3 client and resource from the connector instance\n",
    "s3 = connector.get_s3_resource()\n",
    "s3_client = connector.get_s3_client()\n",
    "buckets = connector.list_buckets()\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "input_dir = os.path.join(DATASET_DIR, \"input\")\n",
    "output_dir = os.path.join(DATASET_DIR, \"output\")\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bba8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box and date range\n",
    "# Australia\n",
    "# bbox = [146.5, -22.0, 149.5, -20.0]\n",
    "# Central Europe bbox\n",
    "bbox = [3.2833, 45.3833, 11.2, 50.1833]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b583dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_a4eca43c0b7c456c1fd383fca6c4db8c {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_a4eca43c0b7c456c1fd383fca6c4db8c&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_a4eca43c0b7c456c1fd383fca6c4db8c = L.map(\n",
       "                &quot;map_a4eca43c0b7c456c1fd383fca6c4db8c&quot;,\n",
       "                {\n",
       "                    center: [47.7833, 7.24165],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    ...{\n",
       "  &quot;zoom&quot;: 7,\n",
       "  &quot;zoomControl&quot;: true,\n",
       "  &quot;preferCanvas&quot;: false,\n",
       "}\n",
       "\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_7987e1d08e9f59c8f3648ccf9f48c116 = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {\n",
       "  &quot;minZoom&quot;: 0,\n",
       "  &quot;maxZoom&quot;: 19,\n",
       "  &quot;maxNativeZoom&quot;: 19,\n",
       "  &quot;noWrap&quot;: false,\n",
       "  &quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;,\n",
       "  &quot;subdomains&quot;: &quot;abc&quot;,\n",
       "  &quot;detectRetina&quot;: false,\n",
       "  &quot;tms&quot;: false,\n",
       "  &quot;opacity&quot;: 1,\n",
       "}\n",
       "\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_7987e1d08e9f59c8f3648ccf9f48c116.addTo(map_a4eca43c0b7c456c1fd383fca6c4db8c);\n",
       "        \n",
       "    \n",
       "            map_a4eca43c0b7c456c1fd383fca6c4db8c.fitBounds(\n",
       "                [[45.3833, 3.2833], [50.1833, 11.2]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            var rectangle_7a05fedc1f99d4beceed0cc86b8b435d = L.rectangle(\n",
       "                [[45.3833, 3.2833], [50.1833, 11.2]],\n",
       "                {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;red&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;red&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;noClip&quot;: false, &quot;opacity&quot;: 1.0, &quot;smoothFactor&quot;: 1.0, &quot;stroke&quot;: true, &quot;weight&quot;: 3}\n",
       "            ).addTo(map_a4eca43c0b7c456c1fd383fca6c4db8c);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x750e6f396900>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "# Calculate the center of the bbox\n",
    "center_longitude = (bbox[0] + bbox[2]) / 2\n",
    "center_latitude = (bbox[1] + bbox[3]) / 2\n",
    "\n",
    "# Create a Folium map centered on the bbox center\n",
    "m = folium.Map(location=[center_latitude, center_longitude], zoom_start=7)\n",
    "\n",
    "# Fit the map to the bbox\n",
    "m.fit_bounds([[bbox[1], bbox[0]], [bbox[3], bbox[2]]])\n",
    "\n",
    "# Draw the bbox as a rectangle on the map\n",
    "folium.Rectangle([(bbox[1], bbox[0]), (bbox[3], bbox[2])],\n",
    "                 color='red',\n",
    "                 fill=True,\n",
    "                 fill_color='red',\n",
    "                 fill_opacity=0.2).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee849f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query parameters:\n",
      "Bounding box: [3.2833, 45.3833, 11.2, 50.1833]\n",
      "Date range: 2025-01-01 to 2025-01-15\n",
      "Max items per request: 1000\n",
      "Max cloud cover: 100%\n",
      "L1C Items for 2025-01-01/2025-01-11: 217\n",
      "L2A Items for 2025-01-01/2025-01-11: 217\n",
      "####\n",
      "L1C Items for 2025-01-11/2025-01-15: 94\n",
      "L2A Items for 2025-01-11/2025-01-15: 94\n",
      "####\n"
     ]
    }
   ],
   "source": [
    "# Set up loguru logger\n",
    "log_filename = f\"{DATASET_DIR}/sentinel_query_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "# Remove the default sink and add custom ones\n",
    "logger.remove()\n",
    "# Add a sink for the file with the format you want\n",
    "logger.add(log_filename, format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
    "# Add a sink for stdout with a simpler format\n",
    "logger.add(lambda msg: print(msg, end=\"\"), colorize=True, format=\"{message}\")\n",
    "\n",
    "start_date = datetime(2025, 1, 1)\n",
    "end_date = datetime(2025, 1, 15)\n",
    "max_items = 1000\n",
    "max_cloud_cover = 100\n",
    "\n",
    "# Log query parameters\n",
    "logger.info(f\"Query parameters:\")\n",
    "logger.info(f\"Bounding box: {bbox}\")\n",
    "logger.info(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "logger.info(f\"Max items per request: {max_items}\")\n",
    "logger.info(f\"Max cloud cover: {max_cloud_cover}%\")\n",
    "# Generate the polygon string from bbox [minx, miny, maxx, maxy]\n",
    "polygon = f\"POLYGON (({bbox[0]} {bbox[1]}, {bbox[0]} {bbox[3]}, {bbox[2]} {bbox[3]}, {bbox[2]} {bbox[1]}, {bbox[0]} {bbox[1]}))\"\n",
    "\n",
    "# Initialize empty lists to store all results\n",
    "all_l1c_results = []\n",
    "all_l2a_results = []\n",
    "\n",
    "# Loop through the date range with a step of 5 days\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    # Calculate the end of the current 5-day interval\n",
    "    next_date = min(current_date + timedelta(days=10), end_date)\n",
    "\n",
    "    # Format the dates as required for the OData query (ISO format with Z for UTC)\n",
    "    start_interval = f\"{current_date.strftime('%Y-%m-%dT00:00:00.000Z')}\"\n",
    "    end_interval = f\"{next_date.strftime('%Y-%m-%dT23:59:59.999Z')}\"\n",
    "\n",
    "    date_interval = f\"{current_date.strftime('%Y-%m-%d')}/{next_date.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        l2a_query_url = create_cdse_query_url(\n",
    "            product_type=\"MSIL2A\",\n",
    "            polygon=polygon,\n",
    "            start_interval=start_interval,\n",
    "            end_interval=end_interval,\n",
    "            max_cloud_cover=max_cloud_cover,\n",
    "            max_items=max_items,\n",
    "            orderby=\"ContentDate/Start\"\n",
    "        )\n",
    "        # Search for Sentinel-2 L2A products for this interval\n",
    "        l2a_json = requests.get(l2a_query_url).json()\n",
    "\n",
    "        # Add interval as metadata to each item\n",
    "        l2a_results = l2a_json.get('value', [])\n",
    "        for item in l2a_results:\n",
    "            item['query_interval'] = date_interval\n",
    "\n",
    "\n",
    "        l1c_query_url = create_cdse_query_url(\n",
    "            product_type=\"MSIL1C\",\n",
    "            polygon=polygon,\n",
    "            start_interval=start_interval,\n",
    "            end_interval=end_interval,\n",
    "            max_cloud_cover=max_cloud_cover,\n",
    "            max_items=max_items,\n",
    "            orderby=\"ContentDate/Start\"\n",
    "        )\n",
    "        # Search for Sentinel-2 L1C products for this interval\n",
    "        l1c_json = requests.get(l1c_query_url).json()\n",
    "\n",
    "        # Add interval as metadata to each item\n",
    "        l1c_results = l1c_json.get('value', [])\n",
    "        for item in l1c_results:\n",
    "            item['query_interval'] = date_interval\n",
    "\n",
    "        # Count L1C products\n",
    "        l1c_count = len(l1c_results)\n",
    "        l2a_count = len(l2a_results)\n",
    "\n",
    "        if l1c_count == l2a_count:\n",
    "            # Append to the overall results list?\n",
    "            all_l1c_results.extend(l1c_results)\n",
    "            all_l2a_results.extend(l2a_results)\n",
    "        else:\n",
    "            logger.warning(f\"Mismatch in counts for {date_interval}: L1C={l1c_count}, L2A={l2a_count}\")\n",
    "            all_l1c_results.extend(l1c_results)\n",
    "            all_l2a_results.extend(l2a_results)\n",
    "\n",
    "        # Print results for this interval\n",
    "        logger.info(f\"L1C Items for {date_interval}: {l1c_count}\")\n",
    "        logger.info(f\"L2A Items for {date_interval}: {l2a_count}\")\n",
    "        logger.info(\"####\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing interval {date_interval}: {str(e)}\")\n",
    "\n",
    "    # Move to the next n-day interval\n",
    "    current_date = next_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694b7ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames from the collected results\n",
    "df_l1c = pd.DataFrame(all_l1c_results)\n",
    "df_l2a = pd.DataFrame(all_l2a_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2201701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrames from the collected results\n",
    "df_l1c = pd.DataFrame(all_l1c_results)\n",
    "df_l2a = pd.DataFrame(all_l2a_results)\n",
    "\n",
    "# Select only the required columns\n",
    "df_l2a = df_l2a[[\"Name\", \"S3Path\", \"Footprint\", \"GeoFootprint\",\"Attributes\"]]\n",
    "df_l1c = df_l1c[[\"Name\", \"S3Path\", \"Footprint\", \"GeoFootprint\",\"Attributes\"]]\n",
    "\n",
    "df_l1c['cloud_cover'] = df_l1c['Attributes'].apply(lambda x: x[2][\"Value\"])\n",
    "df_l2a['cloud_cover'] = df_l2a['Attributes'].apply(lambda x: x[2][\"Value\"])\n",
    "\n",
    "\n",
    "df_l1c.to_csv(f\"{DATASET_DIR}/input_l1c.csv\")\n",
    "df_l2a.to_csv(f\"{DATASET_DIR}/output_l2a.csv\")\n",
    "\n",
    "# Define the function to manipulate the Sentinel ID\n",
    "def remove_last_segment_rsplit(sentinel_id):\n",
    "    # Split from the right side, max 1 split\n",
    "    parts = sentinel_id.rsplit('_', 1)\n",
    "    return parts[0]\n",
    "\n",
    "# Create the id_key column based on the function\n",
    "df_l2a['id_key'] = df_l2a['Name'].apply(remove_last_segment_rsplit)\n",
    "df_l2a['id_key'] = df_l2a['id_key'].str.replace('MSIL2A_', 'MSIL1C_')  # Replace prefix for matching\n",
    "df_l1c['id_key'] = df_l1c['Name'].apply(remove_last_segment_rsplit)\n",
    "\n",
    "# Step 1: Drop duplicates in each DataFrame and keep the first occurrence\n",
    "df_l2a = df_l2a.drop_duplicates(subset='id_key', keep='first')\n",
    "df_l1c = df_l1c.drop_duplicates(subset='id_key', keep='first')\n",
    "\n",
    "# Step 2: Find the common id_keys to ensure both DataFrames have the same order\n",
    "df_l2a = df_l2a[df_l2a['id_key'].isin(df_l1c['id_key'])]\n",
    "df_l1c = df_l1c[df_l1c['id_key'].isin(df_l2a['id_key'])]\n",
    "\n",
    "# Step 3: Align the DataFrames by the order of id_key\n",
    "df_l2a = df_l2a.set_index('id_key')\n",
    "df_l1c = df_l1c.set_index('id_key')\n",
    "\n",
    "df_l2a = df_l2a.loc[df_l1c.index].reset_index()\n",
    "df_l1c = df_l1c.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ab77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1c = df_l1c[0:20]\n",
    "df_l2a = df_l2a[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_l1c = df_l1c.sample(n=1500, random_state=42)\n",
    "# df_l2a = df_l2a.sample(n=1500, random_state=42)\n",
    "# df_l1c = df_l1c.reset_index(drop=True)\n",
    "# df_l2a = df_l2a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028a532",
   "metadata": {},
   "source": [
    "## Generate random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65d2bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T32UMA == S2A_MSIL1C_20250101T104441_N0511_R008_T32UMA\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T32ULA == S2A_MSIL1C_20250101T104441_N0511_R008_T32ULA\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TFM == S2A_MSIL1C_20250101T104441_N0511_R008_T31TFM\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TFL == S2A_MSIL1C_20250101T104441_N0511_R008_T31TFL\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UGR == S2A_MSIL1C_20250101T104441_N0511_R008_T31UGR\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UEP == S2A_MSIL1C_20250101T104441_N0511_R008_T31UEP\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TGL == S2A_MSIL1C_20250101T104441_N0511_R008_T31TGL\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TEL == S2A_MSIL1C_20250101T104441_N0511_R008_T31TEL\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TEM == S2A_MSIL1C_20250101T104441_N0511_R008_T31TEM\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T32TLT == S2A_MSIL1C_20250101T104441_N0511_R008_T32TLT\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UGQ == S2A_MSIL1C_20250101T104441_N0511_R008_T31UGQ\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T32UMV == S2A_MSIL1C_20250101T104441_N0511_R008_T32UMV\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TGN == S2A_MSIL1C_20250101T104441_N0511_R008_T31TGN\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T32ULU == S2A_MSIL1C_20250101T104441_N0511_R008_T32ULU\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UFQ == S2A_MSIL1C_20250101T104441_N0511_R008_T31UFQ\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UFP == S2A_MSIL1C_20250101T104441_N0511_R008_T31UFP\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31TEN == S2A_MSIL1C_20250101T104441_N0511_R008_T31TEN\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UEQ == S2A_MSIL1C_20250101T104441_N0511_R008_T31UEQ\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T31UGP == S2A_MSIL1C_20250101T104441_N0511_R008_T31UGP\n",
      "Match: S2A_MSIL1C_20250101T104441_N0511_R008_T32ULV == S2A_MSIL1C_20250101T104441_N0511_R008_T32ULV\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(len(df_l1c), len(df_l2a))):\n",
    "    if df_l1c['id_key'][i] == df_l2a['id_key'][i]:\n",
    "        print(f\"Match: {df_l1c['id_key'][i]} == {df_l2a['id_key'][i]}\")\n",
    "    else:\n",
    "        print(f\"Mismatch: {df_l1c['id_key'][i]} != {df_l2a['id_key'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4fa82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1c.to_csv(f\"{DATASET_DIR}/sample_input_l1c.csv\")\n",
    "df_l2a.to_csv(f\"{DATASET_DIR}/sample_output_l2a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90b373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "def parse_safe_manifest(content: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parse a Sentinel SAFE manifest file and extract href attributes.\n",
    "\n",
    "    Args:\n",
    "        manifest_path (str): Path to the manifest.safe file\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing href values and file information,\n",
    "                     or None if an error occurred\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        # Parse the content\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        # Extract all elements with an href attribute using a generic approach\n",
    "        hrefs = []\n",
    "        for elem in root.findall(\".//*[@href]\"):\n",
    "            href = elem.get('href')\n",
    "            if href:\n",
    "                hrefs.append(href)\n",
    "\n",
    "        # Create DataFrame with href values and file information\n",
    "        df_files = pd.DataFrame({\n",
    "            'href': hrefs,\n",
    "            'file_type': [href.split('.')[-1] if '.' in href else 'unknown' for href in hrefs],\n",
    "            'file_name': [os.path.basename(href) for href in hrefs]\n",
    "        })\n",
    "\n",
    "\n",
    "        return df_files\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        logger.error(f\"XML parsing error : {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing manifest: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_manifest(s3_client, bucket_name, s3_path, max_attempts=5, retry_delay=2):\n",
    "    \"\"\"\n",
    "    Download and parse a Sentinel-2 product manifest file from S3.\n",
    "\n",
    "    Args:\n",
    "        s3_client: Boto3 S3 client\n",
    "        bucket_name (str): S3 bucket name\n",
    "        s3_path (str): Base S3 path to the product\n",
    "        max_attempts (int): Maximum number of download attempts\n",
    "        retry_delay (int): Seconds to wait between retry attempts\n",
    "\n",
    "    Returns:\n",
    "        tuple: (success (bool), dataframe of files or None)\n",
    "    \"\"\"\n",
    "    # Extract base S3 URL and create manifest URL\n",
    "    s3_base_url = extract_s3_path_from_url(s3_path).replace(\"/eodata\", \"\")\n",
    "    s3_manifest_url = f\"{s3_base_url}/manifest.safe\"\n",
    "\n",
    "    # Try to download manifest file with retry logic\n",
    "    attempt = 0\n",
    "    content = None\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            # Get the manifest file\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=s3_manifest_url)\n",
    "\n",
    "            # Check if successful\n",
    "            if response[\"ResponseMetadata\"]['HTTPStatusCode'] == 200:\n",
    "                content = response['Body'].read()\n",
    "                logger.info(f\"Downloaded manifest from {s3_manifest_url}\")\n",
    "                break\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected status: {response['ResponseMetadata']['HTTPStatusCode']}\")\n",
    "                attempt += 1\n",
    "                time.sleep(retry_delay)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error downloading manifest: {str(e)}\")\n",
    "            attempt += 1\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    if content is None:\n",
    "        logger.error(f\"Failed to download manifest after {max_attempts} attempts\")\n",
    "        return False, None\n",
    "\n",
    "    # Parse the manifest into a dataframe\n",
    "    df_files = parse_safe_manifest(content=content)\n",
    "\n",
    "    return df_files\n",
    "\n",
    "\n",
    "def filter_band_files(df_files, bands=None, product_type=None, resolution=None):\n",
    "    \"\"\"\n",
    "    Filter a dataframe for Sentinel-2 band files supporting both L1C and L2A formats.\n",
    "\n",
    "    Args:\n",
    "        df_files (pd.DataFrame): DataFrame with 'href' column containing file paths\n",
    "        bands (list, optional): List of band names to filter for (e.g., ['B02', 'B03', 'B04']).\n",
    "                               If None, defaults to RGB bands.\n",
    "        product_type (str, optional): Product type ('L1C' or 'L2A'). If None, both types are included.\n",
    "        resolution (str or int, optional): Specific resolution to filter for L2A products ('10m', '20m', '60m' or 10, 20, 60).\n",
    "                                         If None, includes all resolutions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only requested band files\n",
    "    \"\"\"\n",
    "    # Define default bands to filter if not specified\n",
    "    if bands is None:\n",
    "        bands = ['B02', 'B03', 'B04']  # RGB bands by default\n",
    "\n",
    "    # Convert resolution to string if it's an integer\n",
    "    if resolution is not None:\n",
    "        resolution = str(resolution)\n",
    "\n",
    "    # Build regex patterns to match both L1C and L2A formats\n",
    "    band_patterns = []\n",
    "\n",
    "    for band in bands:\n",
    "        # L1C format: IMG_DATA/*_B02.jp2\n",
    "        if product_type is None or product_type.upper() == 'L1C':\n",
    "            band_patterns.append(r'IMG_DATA/.*_' + band + r'\\.jp2')\n",
    "\n",
    "        # L2A formats with correct pattern: IMG_DATA/R20m/T55KGR_20200103T001101_B02_20m.jp2\n",
    "        if product_type is None or product_type.upper() == 'L2A':\n",
    "            if resolution:\n",
    "                # If specific resolution is provided, filter for that resolution\n",
    "                band_patterns.append(r'IMG_DATA/R' + resolution + r'm/.*_' + band + r'_' + resolution + r'm\\.jp2')\n",
    "            else:\n",
    "                # If no resolution is specified, include all resolutions\n",
    "                band_patterns.extend([\n",
    "                    r'IMG_DATA/R10m/.*_' + band + r'_10m\\.jp2',\n",
    "                    r'IMG_DATA/R20m/.*_' + band + r'_20m\\.jp2',\n",
    "                    r'IMG_DATA/R60m/.*_' + band + r'_60m\\.jp2'\n",
    "                ])\n",
    "\n",
    "    filter_condition = False\n",
    "    for pattern in band_patterns:\n",
    "        filter_condition = filter_condition | df_files['href'].str.contains(pattern, regex=True)\n",
    "\n",
    "    df_gr = df_files[filter_condition].copy()  # Create a copy to avoid the warning\n",
    "\n",
    "\n",
    "    # Remove leading ./ from href paths\n",
    "    df_gr['href'] = df_gr['href'].str.replace(r'^\\./', '', regex=True)\n",
    "\n",
    "    return df_gr\n",
    "\n",
    "def get_product_content(s3_client, bucket_name, object_url):\n",
    "    \"\"\"\n",
    "    Download the content of a product from S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        s3_client: boto3 S3 client object\n",
    "        bucket_name (str): Name of the S3 bucket\n",
    "        object_url (str): Path to the object within the bucket\n",
    "\n",
    "    Returns:\n",
    "        bytes: Content of the downloaded file\n",
    "    \"\"\"\n",
    "    print(f\"Downloading {object_url}\")\n",
    "\n",
    "    try:\n",
    "        # Download the file from S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_url)\n",
    "        content = response['Body'].read()\n",
    "        print(f\"Successfully downloaded {object_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return content\n",
    "\n",
    "def get_product(s3_client, bucket_name, object_url, output_path,\n",
    "                           resize=False, target_size=None, interpolation=cv2.INTER_CUBIC,\n",
    "                           format='JPEG'):\n",
    "    \"\"\"\n",
    "    Retrieve a satellite image from S3, optionally resize it, and save it.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    s3_client : boto3.client\n",
    "        Initialized S3 client\n",
    "    bucket_name : str\n",
    "        S3 bucket name\n",
    "    object_url : str\n",
    "        S3 object key/URL for the image\n",
    "    output_path : str\n",
    "        Path where the image should be saved\n",
    "    resize : bool\n",
    "        Whether to resize the image (default: False)\n",
    "    target_size : tuple or None\n",
    "        Target size as (width, height). Required if resize=True.\n",
    "    interpolation : int\n",
    "        OpenCV interpolation method (default: cv2.INTER_CUBIC)\n",
    "    format : str\n",
    "        Output image format (default: 'JPEG')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str : Path to the saved image\n",
    "    \"\"\"\n",
    "    # Get the image content from S3\n",
    "    product_content = get_product_content(s3_client=s3_client,\n",
    "                                          bucket_name=bucket_name,\n",
    "                                          object_url=object_url)\n",
    "\n",
    "    # Open as PIL Image\n",
    "    image = Image.open(io.BytesIO(product_content))\n",
    "\n",
    "    # Only resize if requested\n",
    "    if resize:\n",
    "        if target_size is None:\n",
    "            raise ValueError(\"target_size must be specified when resize=True\")\n",
    "\n",
    "        # Convert to numpy array\n",
    "        image_array = np.array(image)\n",
    "\n",
    "        # Resize using OpenCV\n",
    "        resized_array = cv2.resize(image_array, target_size, interpolation=interpolation)\n",
    "\n",
    "        # Convert back to PIL Image\n",
    "        image = Image.fromarray(resized_array)\n",
    "\n",
    "    # Save the image (resized or original) to the specified path\n",
    "    image.save(output_path, format=format)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def download_bands(s3_client, bucket_name, df, bands, product_type, resolution, output_dir, max_attempts=10, retry_delay=10):\n",
    "    \"\"\"\n",
    "    Download Sentinel-2 band files from S3 based on dataframe information.\n",
    "\n",
    "    Args:\n",
    "        s3_client: S3 client object\n",
    "        bucket: S3 bucket object\n",
    "        bucket_name: Name of the S3 bucket\n",
    "        df (pd.DataFrame): DataFrame with 'S3Path' column containing S3 paths\n",
    "        bands (list): List of bands to download\n",
    "        product_type (str): Product type ('L1C' or 'L2A')\n",
    "        resolution (int, optional): Resolution in meters. Required for L2A products.\n",
    "        output_dir (str): Base directory to save files\n",
    "        max_attempts (int): Maximum number of download attempts\n",
    "        retry_delay (int): Delay between retry attempts in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract base S3 URL\n",
    "        s3_base_url = extract_s3_path_from_url(row['S3Path']).replace(\"/eodata\",\"\")\n",
    "        s3_manifest_url = f\"{s3_base_url}/manifest.safe\"\n",
    "        _, filename = os.path.split(s3_manifest_url)\n",
    "\n",
    "        # Try to download manifest file with retry logic\n",
    "        attempt = 0\n",
    "        content = None\n",
    "\n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                # Get the manifest file\n",
    "                response = s3_client.get_object(Bucket=bucket_name, Key=s3_manifest_url)\n",
    "                # Check if successful\n",
    "                if response[\"ResponseMetadata\"]['HTTPStatusCode'] == 200:\n",
    "                    content = response['Body'].read()\n",
    "                    logger.info(f\"Downloaded manifest from {s3_manifest_url}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected status: {response['ResponseMetadata']['HTTPStatusCode']}\")\n",
    "                    attempt += 1\n",
    "                    time.sleep(retry_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error downloading manifest: {str(e)}\")\n",
    "                attempt += 1\n",
    "                time.sleep(retry_delay)\n",
    "\n",
    "        if content is None:\n",
    "            logger.error(f\"Failed to download manifest after {max_attempts} attempts, skipping this product\")\n",
    "            continue\n",
    "\n",
    "        df_tmp = parse_safe_manifest(content=content)\n",
    "        df_bands = filter_band_files(df_tmp, bands=bands, product_type=product_type, resolution=resolution)\n",
    "\n",
    "        for gr in df_bands['href']:\n",
    "            # Create full S3 URL for the band file\n",
    "            band_s3_url = f\"{s3_base_url}/{gr}\"\n",
    "\n",
    "            # Extract just the filename from the path\n",
    "            filename = os.path.basename(gr)\n",
    "\n",
    "            # Extract product ID for folder structure\n",
    "            path_safe = s3_base_url.split(os.sep)[7].replace(\".SAFE\",\"\")\n",
    "            path_save = os.path.join(output_dir, path_safe)\n",
    "            os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "            # Download the file with retry logic\n",
    "            attempt = 0\n",
    "\n",
    "            while attempt < max_attempts:\n",
    "                try:\n",
    "                    # Download the band file\n",
    "                    # bucket.download_file(band_s3_url, f\"{path_save}/{filename}\")\n",
    "\n",
    "                    output_path = f\"{path_save}/{filename}\"\n",
    "                    get_product(s3_client=s3_client, bucket_name=bucket_name,\n",
    "                                object_url=band_s3_url, output_path=output_path,\n",
    "                           resize=False, target_size=(1830, 1830), interpolation=cv2.INTER_CUBIC,\n",
    "                           format='PNG')\n",
    "                    logger.info(f\"Downloaded {filename} to {path_save}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error downloading band file {filename}: {str(e)}\")\n",
    "                    attempt += 1\n",
    "                    if attempt < max_attempts:\n",
    "                        logger.info(f\"Retrying download of {filename}, attempt {attempt+1} of {max_attempts}\")\n",
    "                        time.sleep(retry_delay)\n",
    "                    else:\n",
    "                        logger.error(f\"Failed to download {filename} after {max_attempts} attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c308ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_filename = f\"{DATASET_DIR}/sentinel_download_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "# Remove the default sink and add custom ones\n",
    "logger.remove()\n",
    "# Add a sink for the file with the format you want\n",
    "logger.add(log_filename, format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\")\n",
    "# Add a sink for stdout with a simpler format\n",
    "logger.add(lambda msg: print(msg, end=\"\"), colorize=True, format=\"{message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23ad21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_bands(s3_client=s3_client, bucket_name=BUCKET_NAME, df=df_l1c[:1],\n",
    "#                 product_type=\"L1C\", bands=BANDS, resolution=None, output_dir=input_dir,\n",
    "#                 max_attempts=10, retry_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ebc90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_DIR = \"/mnt/disk/dataset/sentinel-ai-processor/V1\"\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# df1 = pd.read_csv(f\"{DATASET_DIR}/input_l1c.csv\")\n",
    "# df2 = pd.read_csv(f\"{DATASET_DIR}/output_l2a.csv\")\n",
    "\n",
    "# df_l1c = df1.sample(n=15000, random_state=42)\n",
    "# df_l2a = df2.sample(n=15000, random_state=42)\n",
    "# df_l1c = df_l1c.reset_index(drop=True)\n",
    "# df_l2a = df_l2a.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
