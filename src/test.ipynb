{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78165a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ai_processor/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import natsort\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from data.dataset import Sentinel2TCIDataset, Sentinel2Dataset\n",
    "from data.loader import define_loaders\n",
    "from model_zoo.models import define_model\n",
    "from training.metrics import MultiSpectralMetrics, avg_metric_bands\n",
    "from utils.torch import count_parameters, load_model_weights, seed_everything\n",
    "from utils.utils import load_config\n",
    "from utils.wandb_logger import WandbLogger\n",
    "from training.losses import WeightedMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451795ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_paths(path_dir):\n",
    "\n",
    "\n",
    "    df_input = pd.read_csv(f\"{path_dir}/input.csv\")\n",
    "    df_output = pd.read_csv(f\"{path_dir}/target.csv\")\n",
    "\n",
    "    df_input[\"path\"] = df_input[\"Name\"].apply(lambda x: os.path.join(path_dir, \"input\", os.path.basename(x).replace(\".SAFE\",\"\")))\n",
    "    df_output[\"path\"] = df_output[\"Name\"].apply(lambda x: os.path.join(path_dir, \"target\", os.path.basename(x).replace(\".SAFE\",\"\")))\n",
    "\n",
    "    return df_input, df_output\n",
    "\n",
    "\n",
    "def prepare_data(config):\n",
    "    base_dir = config['DATASET']['base_dir']\n",
    "    version = config['DATASET']['version']\n",
    "    resize = config['TRAINING']['resize']\n",
    "\n",
    "\n",
    "    TRAIN_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/train/\"\n",
    "    VAL_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/val/\"\n",
    "    TEST_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/test/\"\n",
    "\n",
    "    df_train_input, df_train_output =  prepare_paths(TRAIN_DIR)\n",
    "    df_val_input, df_val_output =  prepare_paths(VAL_DIR)\n",
    "    df_test_input, df_test_output =  prepare_paths(TEST_DIR)\n",
    "\n",
    "    logger.info(f\"Number of training samples: {len(df_train_input)}\")\n",
    "    logger.info(f\"Number of validation samples: {len(df_val_input)}\")\n",
    "    logger.info(f\"Number of test samples: {len(df_test_input)}\")\n",
    "\n",
    "    train_dataset = Sentinel2Dataset(df_x=df_train_input, df_y=df_train_output, train=True, augmentation=False, img_size=resize)\n",
    "    val_dataset = Sentinel2Dataset(df_x=df_val_input, df_y=df_val_output, train=True, augmentation=False, img_size=resize)\n",
    "    test_dataset = Sentinel2Dataset(df_x=df_test_input, df_y=df_test_output, train=True, augmentation=False, img_size=resize)\n",
    "\n",
    "\n",
    "\n",
    "    train_loader, val_loader = define_loaders(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        train=True,\n",
    "        batch_size=config['TRAINING']['batch_size'],\n",
    "        num_workers=config['TRAINING']['num_workers'])\n",
    "\n",
    "    test_loader = define_loaders(\n",
    "        train_dataset=test_dataset,\n",
    "        val_dataset=None,\n",
    "        train=False,\n",
    "        batch_size=config['TRAINING']['batch_size'],\n",
    "        num_workers=config['TRAINING']['num_workers'])\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6618081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(config_path=\"cfg/config.yaml\")\n",
    "version = \"V3\"\n",
    "resize = 1024\n",
    "TEST_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/test/\"\n",
    "df_input, df_output = prepare_paths(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e10bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_array):\n",
    "    \"\"\"\n",
    "    Normalize each band in a multi-band image array to the range [0, 1].\n",
    "\n",
    "    For each band, pixels with values > 0 are considered valid.\n",
    "    Normalization is done based on the min and max of valid pixels only.\n",
    "    Invalid pixels (â‰¤ 0) are set to 0 after normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_array : np.ndarray\n",
    "        Input image array of shape (H, W, C), where H = height, W = width, C = number of bands.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    normalized_data : np.ndarray\n",
    "        Normalized data array with values in [0, 1], same shape as input.\n",
    "    valid_masks : np.ndarray\n",
    "        Boolean mask array indicating valid pixels, shape (H, W, C).\n",
    "    norm_params : list of tuple\n",
    "        List of (min, max) values used for normalization per band.\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_data = []\n",
    "    valid_masks = []\n",
    "    norm_params = []\n",
    "    for i in range(data_array.shape[2]):\n",
    "        band_data = data_array[:, :, i]\n",
    "        valid_mask = (band_data > 0)\n",
    "        valid_pixels = band_data[valid_mask]\n",
    "        min_val = np.min(valid_pixels)\n",
    "        max_val = np.max(valid_pixels)\n",
    "        norm_params.append((min_val, max_val))\n",
    "\n",
    "        result = band_data.copy().astype(np.float32)\n",
    "        result[valid_mask] = (valid_pixels - min_val) / (max_val - min_val)\n",
    "        result[~valid_mask] = 0.0\n",
    "        normalized_data.append(result)\n",
    "        valid_masks.append(valid_mask)\n",
    "    return np.dstack(normalized_data), np.dstack(valid_masks), norm_params\n",
    "\n",
    "def denormalize(norm_data, valid_masks, norm_params):\n",
    "    \"\"\"\n",
    "    Denormalize a normalized multi-band image back to its original value range and convert to integers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    norm_data : ndarray\n",
    "        A 3D NumPy array of normalized data (H, W, C), where values are in the range [0, 1].\n",
    "\n",
    "    valid_masks : ndarray\n",
    "        Boolean mask array of shape (H, W, C) indicating which pixels were originally valid.\n",
    "\n",
    "    norm_params : list of tuples\n",
    "        List of (min, max) values per band used during normalization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    restored_data : ndarray\n",
    "        Denormalized image array of shape (H, W, C) with dtype uint16.\n",
    "    \"\"\"\n",
    "\n",
    "    restored_data = []\n",
    "    for i in range(norm_data.shape[2]):\n",
    "        band = norm_data[:, :, i]\n",
    "        valid_mask = valid_masks[:, :, i]\n",
    "        min_val, max_val = norm_params[i]\n",
    "\n",
    "        restored_band = band.copy()\n",
    "        restored_band[valid_mask] = band[valid_mask] * (max_val - min_val) + min_val\n",
    "\n",
    "        restored_band[~valid_mask] = 0.0\n",
    "        restored_data.append(np.round(restored_band).astype(np.uint16))\n",
    "    return np.dstack(restored_data)\n",
    "\n",
    "def read_images(product_paths):\n",
    "    \"\"\"\n",
    "    Read and stack a list of grayscale image files into a multi-band image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    product_paths : list of str\n",
    "        List of file paths to the images to read.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : ndarray\n",
    "        A 3D NumPy array of shape (H, W, C), where each image is treated as one band (C channels).\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    for path in product_paths:\n",
    "        data = Image.open(path)\n",
    "        data = np.array(data)\n",
    "        images.append(data)\n",
    "    # H x W x C\n",
    "    images = np.dstack(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d20eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.choice(df_input.index)\n",
    "random_row = df_input.loc[random_index]\n",
    "\n",
    "x_paths = natsort.natsorted(glob.glob(os.path.join(df_input[\"path\"][random_index], \"*.png\")))\n",
    "y_paths = natsort.natsorted(glob.glob(os.path.join(df_output[\"path\"][random_index], \"*.png\")))\n",
    "\n",
    "x_data = read_images(x_paths)\n",
    "y_data = read_images(y_paths)\n",
    "\n",
    "x_data_norm, x_valid_masks, x_norm_params = normalize(x_data)\n",
    "y_data_norm, y_valid_masks, y_norm_params = normalize(y_data)\n",
    "\n",
    "new_x_data = denormalize(x_data_norm, x_valid_masks, x_norm_params)\n",
    "new_y_data = denormalize(y_data_norm, y_valid_masks, y_norm_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
