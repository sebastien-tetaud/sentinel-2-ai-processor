{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78165a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import natsort\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from data.dataset import Sentinel2TCIDataset, Sentinel2Dataset\n",
    "from data.loader import define_loaders\n",
    "from model_zoo.models import define_model\n",
    "from training.metrics import MultiSpectralMetrics, avg_metric_bands\n",
    "from utils.torch import count_parameters, load_model_weights, seed_everything\n",
    "from utils.utils import load_config\n",
    "from utils.wandb_logger import WandbLogger\n",
    "from training.losses import WeightedMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "451795ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_paths(path_dir):\n",
    "\n",
    "\n",
    "    df_input = pd.read_csv(f\"{path_dir}/input.csv\")\n",
    "    df_output = pd.read_csv(f\"{path_dir}/target.csv\")\n",
    "\n",
    "    df_input[\"path\"] = df_input[\"Name\"].apply(lambda x: os.path.join(path_dir, \"input\", os.path.basename(x).replace(\".SAFE\",\"\")))\n",
    "    df_output[\"path\"] = df_output[\"Name\"].apply(lambda x: os.path.join(path_dir, \"target\", os.path.basename(x).replace(\".SAFE\",\"\")))\n",
    "\n",
    "    return df_input, df_output\n",
    "\n",
    "\n",
    "def prepare_data(config):\n",
    "    base_dir = config['DATASET']['base_dir']\n",
    "    version = config['DATASET']['version']\n",
    "    resize = config['TRAINING']['resize']\n",
    "\n",
    "\n",
    "    TRAIN_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/train/\"\n",
    "    VAL_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/val/\"\n",
    "    TEST_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/test/\"\n",
    "\n",
    "    df_train_input, df_train_output =  prepare_paths(TRAIN_DIR)\n",
    "    df_val_input, df_val_output =  prepare_paths(VAL_DIR)\n",
    "    df_test_input, df_test_output =  prepare_paths(TEST_DIR)\n",
    "\n",
    "    logger.info(f\"Number of training samples: {len(df_train_input)}\")\n",
    "    logger.info(f\"Number of validation samples: {len(df_val_input)}\")\n",
    "    logger.info(f\"Number of test samples: {len(df_test_input)}\")\n",
    "\n",
    "    train_dataset = Sentinel2Dataset(df_x=df_train_input, df_y=df_train_output, train=True, augmentation=False, img_size=resize)\n",
    "    val_dataset = Sentinel2Dataset(df_x=df_val_input, df_y=df_val_output, train=True, augmentation=False, img_size=resize)\n",
    "    test_dataset = Sentinel2Dataset(df_x=df_test_input, df_y=df_test_output, train=True, augmentation=False, img_size=resize)\n",
    "\n",
    "\n",
    "\n",
    "    train_loader, val_loader = define_loaders(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        train=True,\n",
    "        batch_size=config['TRAINING']['batch_size'],\n",
    "        num_workers=config['TRAINING']['num_workers'])\n",
    "\n",
    "    test_loader = define_loaders(\n",
    "        train_dataset=test_dataset,\n",
    "        val_dataset=None,\n",
    "        train=False,\n",
    "        batch_size=config['TRAINING']['batch_size'],\n",
    "        num_workers=config['TRAINING']['num_workers'])\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6618081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(config_path=\"cfg/config.yaml\")\n",
    "version = \"V3\"\n",
    "resize = 1024\n",
    "TEST_DIR = f\"/mnt/disk/dataset/sentinel-ai-processor/{version}/test/\"\n",
    "df_input, df_output = prepare_paths(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e10bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_array):\n",
    "    \"\"\"\n",
    "    Normalize each band in a multi-band image array to the range [0, 1].\n",
    "\n",
    "    For each band, pixels with values > 0 are considered valid.\n",
    "    Normalization is done based on the min and max of valid pixels only.\n",
    "    Invalid pixels (â‰¤ 0) are set to 0 after normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_array : np.ndarray\n",
    "        Input image array of shape (H, W, C), where H = height, W = width, C = number of bands.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    normalized_data : np.ndarray\n",
    "        Normalized data array with values in [0, 1], same shape as input.\n",
    "    valid_masks : np.ndarray\n",
    "        Boolean mask array indicating valid pixels, shape (H, W, C).\n",
    "    norm_params : list of tuple\n",
    "        List of (min, max) values used for normalization per band.\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_data = []\n",
    "    valid_masks = []\n",
    "    norm_params = []\n",
    "    for i in range(data_array.shape[2]):\n",
    "        band_data = data_array[:, :, i]\n",
    "        valid_mask = (band_data > 0)\n",
    "        valid_pixels = band_data[valid_mask]\n",
    "        min_val = np.min(valid_pixels)\n",
    "        max_val = np.max(valid_pixels)\n",
    "        norm_params.append((min_val, max_val))\n",
    "\n",
    "        result = band_data.copy().astype(np.float32)\n",
    "        result[valid_mask] = (valid_pixels - min_val) / (max_val - min_val)\n",
    "        result[~valid_mask] = 0.0\n",
    "        normalized_data.append(result)\n",
    "        valid_masks.append(valid_mask)\n",
    "    return np.dstack(normalized_data), np.dstack(valid_masks), norm_params\n",
    "\n",
    "def denormalize(norm_data, valid_masks, norm_params):\n",
    "    \"\"\"\n",
    "    Denormalize a normalized multi-band image back to its original value range and convert to integers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    norm_data : ndarray\n",
    "        A 3D NumPy array of normalized data (H, W, C), where values are in the range [0, 1].\n",
    "\n",
    "    valid_masks : ndarray\n",
    "        Boolean mask array of shape (H, W, C) indicating which pixels were originally valid.\n",
    "\n",
    "    norm_params : list of tuples\n",
    "        List of (min, max) values per band used during normalization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    restored_data : ndarray\n",
    "        Denormalized image array of shape (H, W, C) with dtype uint16.\n",
    "    \"\"\"\n",
    "\n",
    "    restored_data = []\n",
    "    for i in range(norm_data.shape[2]):\n",
    "        band = norm_data[:, :, i]\n",
    "        valid_mask = valid_masks[:, :, i]\n",
    "        min_val, max_val = norm_params[i]\n",
    "\n",
    "        restored_band = band.copy()\n",
    "        restored_band[valid_mask] = band[valid_mask] * (max_val - min_val) + min_val\n",
    "\n",
    "        restored_band[~valid_mask] = 0.0\n",
    "        restored_data.append(np.round(restored_band).astype(np.uint16))\n",
    "    return np.dstack(restored_data)\n",
    "\n",
    "def read_images(product_paths):\n",
    "    \"\"\"\n",
    "    Read and stack a list of grayscale image files into a multi-band image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    product_paths : list of str\n",
    "        List of file paths to the images to read.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : ndarray\n",
    "        A 3D NumPy array of shape (H, W, C), where each image is treated as one band (C channels).\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    for path in product_paths:\n",
    "        data = Image.open(path)\n",
    "        data = np.array(data)\n",
    "        images.append(data)\n",
    "    # H x W x C\n",
    "    images = np.dstack(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d20eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.choice(df_input.index)\n",
    "random_row = df_input.loc[random_index]\n",
    "\n",
    "x_paths = natsort.natsorted(glob.glob(os.path.join(df_input[\"path\"][random_index], \"*.png\")))\n",
    "y_paths = natsort.natsorted(glob.glob(os.path.join(df_output[\"path\"][random_index], \"*.png\")))\n",
    "\n",
    "x_data = read_images(x_paths)\n",
    "y_data = read_images(y_paths)\n",
    "\n",
    "x_data_norm, x_valid_masks, x_norm_params = normalize(x_data)\n",
    "y_data_norm, y_valid_masks, y_norm_params = normalize(y_data)\n",
    "\n",
    "new_x_data = denormalize(x_data_norm, x_valid_masks, x_norm_params)\n",
    "new_y_data = denormalize(y_data_norm, y_valid_masks, y_norm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca1c111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_data/15000\n",
    "y = y_data/15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711367ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17566666666666667,\n",
       " 1.8700666666666668,\n",
       " 0.33114022374842794,\n",
       " 0.1107793615294789)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min(), x.max(), x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "9cc8365c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.8224, 0.17187661015059663, 0.5131447801560758)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min(), y.max(), y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "288b09ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 38224, 1718.766101505967, 5131.44780156076)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.min(), y_data.max(), y_data.mean(), y_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5fea5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2708.), tensor(34291.), tensor(9142.5928), tensor(4504.1060))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.min(), x_data.max(), x_data.mean(), x_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2cd5031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 530/530 [03:04<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "l2a_max = []\n",
    "l2a_min = []\n",
    "\n",
    "l1c_max = []\n",
    "l1c_min = []\n",
    "\n",
    "for i in tqdm(range(len(df_input)), desc=\"Processing images\"):\n",
    "    x_paths = natsort.natsorted(glob.glob(os.path.join(df_input[\"path\"][i], \"*.png\")))\n",
    "    y_paths = natsort.natsorted(glob.glob(os.path.join(df_output[\"path\"][i], \"*.png\")))\n",
    "\n",
    "    x_data = read_images(x_paths)\n",
    "    y_data = read_images(y_paths)\n",
    "\n",
    "    x_data = x_data/10000\n",
    "    y_data = y_data/10000\n",
    "\n",
    "    valid_mask = (x_data > 0)\n",
    "    valid_pixels = x_data[valid_mask]\n",
    "\n",
    "    l2a_max.append(x_data.max())\n",
    "    l2a_min.append(valid_pixels.min())\n",
    "\n",
    "    valid_mask = (y_data > 0)\n",
    "    valid_pixels = y_data[valid_mask]\n",
    "\n",
    "    l1c_max.append(y_data.max())\n",
    "    l1c_min.append(valid_pixels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2013b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l1c_min' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      3\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m plt.hist(\u001b[43ml1c_min\u001b[49m, bins=\u001b[32m100\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mL1C Min\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.hist(l2a_min, bins=\u001b[32m100\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mL2A Min\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mL1C / L2A Min Histogram\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'l1c_min' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot histogram max and min\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(l1c_min, bins=100, alpha=0.5, label='L1C Min')\n",
    "\n",
    "plt.hist(l2a_min, bins=100, alpha=0.5, label='L2A Min')\n",
    "plt.title('L1C / L2A Min Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(l2a_max, bins=100, alpha=0.5, label='L2A Max')\n",
    "plt.hist(l1c_max, bins=100, alpha=0.5, label='L1C Max')\n",
    "plt.title('L1C / L2A Max Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1c8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39278149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"timm-efficientnet-b2\",  # choose encoder, e.g. mobilenet_v2, efficientnet-b7, resnet50, etc.\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,  # model output channels (number of classes in your dataset)\n",
    "    activation=None,  # activation function (softmax, sigmoid, etc.)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270b8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37f76574",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = cv2.resize(x_data, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "x_data = torch.from_numpy(x_data).float()\n",
    "x_data = torch.permute(x_data, (2, 0, 1))  # HWC to CHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4289d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data/ 10000\n",
    "x_data = x_data.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9fbd684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7749afe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de8d4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51ca7d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(15.1406, grad_fn=<MaxBackward1>),\n",
       " tensor(-21.2954, grad_fn=<MinBackward1>),\n",
       " tensor(-0.3354, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0282, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.max(), outputs.min(), outputs.mean(), outputs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1f697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
